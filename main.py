# -*- coding: utf-8 -*-
"""main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wmOEtF_U-fZAZrVVpw9tgLTDKQAC8d5R

## Load and save the Llama-2-7b model and tokenizer from Hugging Face
"""

from transformers import LlamaTokenizer, LlamaForCausalLM
import transformers
import torch
import torch.nn.functional as F
import json
import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform
import pprint

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch.cuda.empty_cache()

access_token = "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"
model = "meta-llama/Llama-2-7b-hf"

# !nvidia-smi

## Load tokenizer
llama_tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", token=access_token)

## Load model
llama_model = LlamaForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    token=access_token,
    output_hidden_states=True,
    output_attentions=True).to(device)

"""## For each target concept, get top layer representation from many sentences


"""

inputs_filepath = '/n/home09/lschrage/projects/llama/data_lm/short_inputs.json'

with open(inputs_filepath, 'r') as file:
    inputs_dict = json.load(file)

## Run model
outputdict = {}
prob_toplayerdict = {}
probsdict = {}
word_toplayerdict = {}

WORD_THRESHOLD = [5, 50, 100]
WORD_THRESHOLD_NAMES = {5: 'top_5_words', 50: 'top_50_words', 100: 'top_100_words'}

PROBS_THRESHOLD = [10, 20, 50]
PROBS_THRESHOLD_NAMES = {10: 'top_10_perc', 20: 'top_20_perc', 50: 'top_50_perc'}
for key in inputs_dict:

  target_word = key
  tokens_target_word = llama_tokenizer.tokenize(target_word)
  target_word_id =  llama_tokenizer.convert_tokens_to_ids(tokens_target_word)
  #print('Target word: ', target_word)
  #print('Target word id: ', target_word_id)

  outputs = []
  prob_toplayers = {PROBS_THRESHOLD_NAMES[p]: [] for p in PROBS_THRESHOLD}  # Initialize as a dictionary
  probs = []
  word_toplayers = {WORD_THRESHOLD_NAMES[w]: [] for w in WORD_THRESHOLD}  # Initialize as a dictionary


  for input in inputs_dict[key]:

    tokens = llama_tokenizer(input, return_tensors="pt").to(device)
    output=llama_model.forward(**tokens, return_dict = True)
    outputs.append(output)
    #print('User input: ', input)
    #print('Tokens: ', tokens['input_ids'][0])

    logits = output['logits'][0][-1]
    probabilities = F.softmax(logits, dim=-1)
    final_layer_hidden_states = output['hidden_states'][-1]
    last_hidden_state_for_last_token = final_layer_hidden_states[0, -1, :]
    prob_target_word = probabilities[target_word_id].item()*100
    probs.append(probabilities)

    ###

    for p_threshold in PROBS_THRESHOLD:
      if prob_target_word > p_threshold:
        prob_toplayers[PROBS_THRESHOLD_NAMES[p_threshold]].append(last_hidden_state_for_last_token)
    ###
    sorted_probabilities, sorted_indices = torch.sort(probabilities, descending=True)
    top_preds = llama_tokenizer.convert_ids_to_tokens(sorted_indices)

    for w_threshold in WORD_THRESHOLD:
      top_x_words = top_preds[:w_threshold]
      if '‚ñÅ' + str(key) in top_x_words:
        word_toplayers[WORD_THRESHOLD_NAMES[w_threshold]].append(last_hidden_state_for_last_token)
    ###
    #print('Logits: ', logits)
    #print('Probs: ', probabilities)
    #print('Top layer representation for last token: ', last_hidden_state_for_last_token)
    #print(f'Probability next word is {key}: {prob_target_word}%')
    #print()

  ## outputs for every sentence for every key
  outputdict[key] = outputs

  ## probabilities for vocabulary for every sentence for every key
  probsdict[key] = probs

  ## top layer representations for every sentence for every key (PERCENT PROBABILITY)
  prob_toplayerdict[key] = prob_toplayers

  ## top layer representations for every sentence for every key (TOP X NUMBER WORDS)
  word_toplayerdict[key] = word_toplayers

## force list sizes to be the same accross thresholds
threshold_min_sizes = {threshold: float('inf') for threshold in WORD_THRESHOLD_NAMES.values()}

for key in word_toplayerdict:
    for threshold in WORD_THRESHOLD_NAMES.values():
        if threshold in word_toplayerdict[key]:
            current_length = len(word_toplayerdict[key][threshold])
            if current_length < threshold_min_sizes[threshold]:
                threshold_min_sizes[threshold] = current_length

for key in word_toplayerdict:
    for threshold, min_size in threshold_min_sizes.items():
        if threshold in word_toplayerdict[key] and min_size != float('inf'):
            word_toplayerdict[key][threshold] = word_toplayerdict[key][threshold][:min_size]

print(len(word_toplayerdict['dog']['top_5_words']))
print(len(word_toplayerdict['apple']['top_5_words']))

print(len(word_toplayerdict['dog']['top_10_words']))
print(len(word_toplayerdict['apple']['top_10_words']))

print(len(word_toplayerdict['dog']['top_20_words']))
print(len(word_toplayerdict['apple']['top_20_words']))

## store top layer representations in json dictionary
## WORD TOP LAYERS
np_dict = {
    outer_key: {
        inner_key: [tensor.cpu().detach().numpy().tolist() for tensor in inner_value]
        for inner_key, inner_value in outer_value.items()
    }
    for outer_key, outer_value in word_toplayerdict.items()
}

## PROB TOP LAYERS
'''
np_dict = {
    outer_key: {
        inner_key: [tensor.cpu().detach().numpy().tolist() for tensor in inner_value]
        for inner_key, inner_value in outer_value.items()
    }
    for outer_key, outer_value in prob_toplayerdict.items()
}
'''

data_dict = {}

for key in np_dict:
    data_dict[f'{key}'] = np.array(np_dict[key]).T.tolist()

output_filepath = '/n/home09/lschrage/projects/llama/outputs'

with open(output_filepath, 'w') as f:
    json.dump(data_dict, f)

"""##Calculate Manifold Geometry"""

with open('/n/home09/lschrage/projects/llama/outputs', 'r') as f:
    manifolds = json.load(f)

manifolds['dog'].keys()

def compute_geometry(manifolds):
    geometry = {}
    for key, inner_dict in manifolds.items():
        geometry[key] = {}
        for subkey, data in inner_dict.items():
            data_array = np.array(data)
            centroid = data_array.mean(axis=0)
            norm_manifold = data_array - centroid

            U, S, Vh = np.linalg.svd(norm_manifold, full_matrices=False)
            mean_squared_radius = np.mean(S**2)

            geometry[key][subkey] = {
                'center': centroid,
                'Rs': S,
                'Us': U,
                'mean_squared_radius': mean_squared_radius
            }
    return geometry

geometry = compute_geometry(manifolds)

first_outer_key = next(iter(geometry))
threshold_keys = geometry[first_outer_key].keys()

centers = {key: [] for key in threshold_keys}
Rs = {key: [] for key in threshold_keys}
Us = {key: [] for key in threshold_keys}
MSR = {key: [] for key in threshold_keys}

for outer_key, inner_dicts in geometry.items():
    for threshold, attributes in inner_dicts.items():
        centers[threshold].append(attributes['center'])
        Rs[threshold].append(attributes['Rs'])
        Us[threshold].append(attributes['Us'])
        MSR[threshold].append(attributes['mean_squared_radius'])

dists = {}
dists_norm = {}
dsvds = {}
bias = {}
signal = {}
bias = {}
signal = {}

for key in centers.keys():
    matrix = squareform(pdist(centers[key]))
    dists[key] = matrix

    Rs_array = np.array(Rs[key])
    norm_factor = np.sqrt((Rs_array**2).mean())
    dists_norm[key] = matrix / norm_factor

    l = []
    for r in Rs_array:
      total_variance_squared = np.sum(r**2)**2
      sum_fourth_powers = np.sum(r**4)
      l.append(total_variance_squared / sum_fourth_powers)
    dsvds[key] = l

    m = []
    n = []
    for r in np.array(Rs[key]):
      m.append((r**2).sum(-1))
      n.append((r**2).sum(-1))
    m = np.array(m)
    n = np.array(n)
    bias[key] = m/n[:,None] - 1

    signal[key] = dists_norm[key]**2 + bias[key]

pp = pprint.PrettyPrinter(indent=4)
print("Distances:")
pp.pprint(dists)
print("\nNormalized Distances:")
pp.pprint(dists_norm)
print("\nDsvds (Participation Ratio):")
pp.pprint(dsvds)
print("\nBiases:")
pp.pprint(bias)
print("\nSignals:")
pp.pprint(signal)

"""## Todo:

1. How to include more inputs without memory constraint
2. Maybe instead of set percentage threshold, the target word has to be in the top 5 (or whatever) most liekly to be predicted next words
3. Use SVD on covariance matrix to find singular values and eigenvectors of datamatrix


-- compare centroids
compare different embedinga
compare how dimensionality, radius, signal, noise change based on threshold

look at readout vectors, different embeddings

-- database of sentences
-- same word with different meanings

## Extra code
"""

# % Variance for each eigenvalue
for key in geometry:
  for threshold in geometry[key]:
    new_Rs = np.array(geometry[key][threshold]['Rs'])
    print(f"Percent variance explained by each eigenvalue for {key} for {threshold}:")
    variance_explained_svd = (new_Rs**2) / np.sum(new_Rs**2)*100
    print(variance_explained_svd)
    print('')

for key in probsdict:
  for probs in probsdict[key]:
    sorted_probabilities, sorted_indices = torch.sort(probs, descending=True)
    top_preds = llama_tokenizer.convert_ids_to_tokens(sorted_indices)
    top_20_words = top_preds[:20]

    print("Sorted probabilities:", sorted_probabilities)
    print("Indices of sorted probabilities:", sorted_indices)
    print("20 most likely next words:", top_20_words)
    print()

# Calculate the centroids

def calculate_centroids(data_dict):
    centroids = {}
    for key, points in manifolds.items():
        points_array = np.array(points)
        centroid = np.mean(points_array, axis=1)
        centroids[key] = centroid.tolist()
    return centroids

centroids = calculate_centroids(manifolds)

# Covariance matrix
X_dog = np.array(manifolds['dog'])
X_apple = np.array(manifolds['apple'])

C_dog = (1/X_dog.shape[1])*X_dog@X_dog.T - (np.array(centroids['dog'])@np.array(centroids['dog']).T)
C_apple = (1/X_apple.shape[1])*X_apple@X_apple.T - (np.array(centroids['apple'])@np.array(centroids['apple']).T)
C_apple

## Eigenvalues, eigenvectors of cov matrix

#C_dog_eigenval, C_dog_eigenvec = np.linalg.eig(C_dog)
#C_apple_eigenval, C_apple_eigenvec = np.linalg.eig(C_apple)

#C_dog_eigenval
